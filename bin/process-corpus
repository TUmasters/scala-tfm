#!/usr/bin/env python3

## CLI

import os, argparse

parser = argparse.ArgumentParser(description='Parses a series of text documents into a better format.')
parser.add_argument('data_file', type=str, metavar='D', help='file with documents.')
args = parser.parse_args()

## Load documents

data_path = os.path.dirname(args.data_file)

input_file = args.data_file
output_file = data_path + "/documents.json"

## Construct conversation tree

from _document import parse_corpus, find_roots, prune
import json

with open(input_file, 'r') as f:
    document_data = json.load(f)

corpus = parse_corpus(document_data)
roots = find_roots(corpus)

invalid_content = ["[deleted]", "[removed]"]
invalid_authors = ["AutoModerator", "DeltaBot", "[deleted]", "[removed]"]

to_prune = [d for d in corpus if d.content in invalid_content or d.author in invalid_authors]

corpus = prune(corpus, to_prune)
roots = find_roots(corpus)
print("{} conversations, {} documents".format(len(roots), len(corpus)))

from nltk.tokenize import sent_tokenize
from nltk.tag.stanford import StanfordPOSTagger
from nltk.parse.stanford import StanfordParser, StanfordDependencyParser

tagger = StanfordPOSTagger('english-bidirectional-distsim.tagger')
dep_parser = StanfordDependencyParser() # model_path='edu/stanford/nlp/models/parser/nndep/english_UD.gz'

import random, itertools

def split_document(content):
    lines = content.split('\n')
    sentences = list(itertools.chain(*[sent_tokenize(line) for line in lines if line]))
    return sentences

def test_document():
    document = random.choice(corpus)
    print(document)

    sentences = split_document(document.content)
    print(sentences)
    tag = tagger.tag(sentences)
    parses = [list(parse)[0] for parse in dep_parser.parse_sents(tag)]

    parses[0].tree().draw()
