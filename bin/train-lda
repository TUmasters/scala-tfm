#!/usr/bin/env python3

import argparse
import json
import numpy as np
import os, shutil
from _document import parse_corpus
import random
from gensim import corpora
import itertools

def load_corpus(corpus_file):
    print('Loading...')
    if not os.path.exists(corpus_file):
        print("Could not find file '{}'.".format(corpus_file))
        return
    with open(corpus_file, 'r') as f:
        data = json.load(f)

    for item in data:
        item['reply_to'] = item['parent']
        item['parent'] = None

    corpus = parse_corpus(data)

    print('Vectorizing...')
    d = corpora.Dictionary([doc.words for doc in corpus])
    # print(d.token2id)
    X = [d.doc2bow(doc.words) for doc in corpus]

    # from sklearn.feature_extraction.text import CountVectorizer
    # vectorizer = CountVectorizer()
    # X = vectorizer.fit_transform([str.join(' ', document.words) for document in corpus])
    # n = len(data)
    return corpus, d, X

## Process documents
def run_depth_tests(corpus_file, num_topics):
    corpus, dictionary, X = load_corpus(corpus_file)

    for depth in range(0, 11):
        idx_d = set([index for index, document in enumerate(corpus) if document.level <= depth])
        # X_d = X[idx_d, :]
        X_test = [X[i] for i in range(len(corpus)) if not i in idx_d]
        X_train = [X[i] for i in idx_d]
        # print("depth {}".format(depth))
        run_lda(X_train, X_test, num_topics, dictionary=dictionary)

def run_lda(X_train, X_test, num_topics, num_trials=1, dictionary=None):
    # from sklearn.decomposition import LatentDirichletAllocation
    from gensim.models.ldamulticore import LdaMulticore
    from gensim.models.ldamodel import LdaModel
    from gensim.models.hdpmodel import HdpModel
    import logging

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    train_size = len(X_train)
    test_size = len(X_test)

    print("num topics: {:6d}".format(num_topics))
    print("train: {:6d} test: {:6d}".format(train_size, test_size))

    p1 = []
    p2 = []
    for trial in range(num_trials):
        print("  trial {}".format(trial+1))
        lda = HdpModel(corpus=X_train, id2word=dictionary)
        tp1 = lda.log_perplexity(X_train)
        tp2 = lda.log_perplexity(X_test)
        p1 += [tp1]
        p2 += [tp2]
        print("   perplexity:          {:2.8f}".format(tp1))
        print("   left-out perplexity: {:2.8f}".format(tp2))

    print("avg. perplexity:          {:2.8f}".format(np.mean(p1)))
    print("avg. left-out perplexity: {:2.8f}".format(np.mean(p2)))


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Trains LDA on textual data.')
    parser.add_argument('corpus_file', type=str, metavar='D', help='file with documents.')
    parser.add_argument('--num-topics', type=int, help='number of documents to train on.', default=2)

    args = parser.parse_args()

    # ## depth tests
    # run_depth_tests(args.corpus_file, args.num_topics)

    ## topic tests
    corpus, dictionary, X = load_corpus(args.corpus_file)
    random.shuffle(X)
    test_size = 1000
    (X_test, X_train) = (X[:test_size], X[test_size:])
    for num_topics in [1, 2, 3, 4, 5, 10, 20, 50, 100, 200, 500]:
        run_lda(X_train, X_test, num_topics, dictionary=dictionary)
